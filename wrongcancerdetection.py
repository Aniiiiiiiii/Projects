# -*- coding: utf-8 -*-
"""WrongCancerDetection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ghXMeLbXEalte0Ntgsb6n_2u_37ovXVm
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt

# Define the URL for the breast cancer dataset
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data"

# Define column names
column_names = ['ID', 'ClumpThickness', 'CellSize', 'CellShape', 'MargAdhesion', 'EpithCellSize', 'BareNuclei', 'BlandChromatin', 'NormalNucleoli', 'Mitoses', 'Class']

# Read the dataset from the URL
data = pd.read_csv(url, names=column_names)

# Drop the ID column
data.drop('ID', axis=1, inplace=True)

# Replace '?' with missing value indicator
data.replace('?', np.nan, inplace=True)

# Convert columns to numeric
data = data.apply(pd.to_numeric, errors='coerce')

# Drop rows with missing values
data.dropna(inplace=True)

# Split the data into training and testing sets
X = data.drop('Class', axis=1)
y = data['Class']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Build K Nearest Neighbor classifier
knn = KNeighborsClassifier()
knn.fit(X_train, y_train)
knn_y_pred = knn.predict(X_test)
knn_confusion_matrix = confusion_matrix(y_test, knn_y_pred)

# Build Univariate Decision Tree classifier
dt = DecisionTreeClassifier()
dt.fit(X_train, y_train)
dt_y_pred = dt.predict(X_test)
dt_confusion_matrix = confusion_matrix(y_test, dt_y_pred)

# Build Naive Bayes Gaussian classifier
nb = GaussianNB()
nb.fit(X_train, y_train)
nb_y_pred = nb.predict(X_test)
nb_confusion_matrix = confusion_matrix(y_test, nb_y_pred)

# Report confusion matrices
print("K Nearest Neighbor Confusion Matrix:")
print(knn_confusion_matrix)
print("\nUnivariate Decision Tree Confusion Matrix:")
print(dt_confusion_matrix)
print("\nNaive Bayes Gaussian Confusion Matrix:")
print(nb_confusion_matrix)

# Prepare report of misclassified samples by all classifiers
misclassified_samples = []
for i, (true_class, knn_pred, dt_pred, nb_pred) in enumerate(zip(y_test, knn_y_pred, dt_y_pred, nb_y_pred)):
    if true_class != knn_pred and true_class != dt_pred and true_class != nb_pred:
        misclassified_samples.append((i, true_class, knn_pred, dt_pred, nb_pred))

# Print report of misclassified samples
print("\nMisclassified Samples:")
for sample in misclassified_samples:
    print(f"Index: {sample[0]}, True Class: {sample[1]}, KNN Predicted Class: {sample[2]}, Decision Tree Predicted Class: {sample[3]}, Naive Bayes Predicted Class: {sample[4]}")

# Plot histograms or decision tree as needed
# Plot Decision Tree
plt.figure(figsize=(12, 8))
plot_tree(dt, feature_names=X.columns, class_names=['Benign', 'Malignant'], filled=True)
plt.title('Decision Tree')
plt.show()

# Plot histograms (example of plotting ClumpThickness and CellSize)
plt.figure(figsize=(12, 8))
plt.hist(data[data['Class'] == 2]['ClumpThickness'], bins=10, alpha=0.5, label='Benign')
plt.hist(data[data['Class'] == 4]['ClumpThickness'], bins=10, alpha=0.5, label='Malignant')
plt.xlabel('Clump Thickness')
plt.ylabel('Frequency')
plt.title('Histogram of Clump Thickness')
plt.legend()
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import LabelEncoder

# Load the data
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data"
column_names = ['ID', 'ClumpThickness', 'CellSize', 'CellShape', 'MargAdhesion', 'EpithCellSize', 'BareNuclei', 'BlandChromatin', 'NormalNucleoli', 'Mitoses', 'Class']
data = pd.read_csv(url, names=column_names)

# Drop the ID column
data.drop('ID', axis=1, inplace=True)

# Remove rows with missing values (rows containing '?')
data.replace('?', pd.NA, inplace=True)
data.dropna(inplace=True)

# Convert the 'BareNuclei' column to numeric
data['BareNuclei'] = pd.to_numeric(data['BareNuclei'])

# Separate features and target variable
X = data.drop('Class', axis=1)
y = data['Class']

# Encode the target variable 'Class' (assuming it's binary)
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)

# Initialize classifiers
naive_bayes_classifier = GaussianNB()
knn_classifier = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors (k)
decision_tree_classifier = DecisionTreeClassifier(random_state=42)

# Train classifiers
naive_bayes_classifier.fit(X_train, y_train)
knn_classifier.fit(X_train, y_train)
decision_tree_classifier.fit(X_train, y_train)

# Test classifiers and collect confusion matrices
naive_bayes_confusion = confusion_matrix(y_test, naive_bayes_classifier.predict(X_test))
knn_confusion = confusion_matrix(y_test, knn_classifier.predict(X_test))
decision_tree_confusion = confusion_matrix(y_test, decision_tree_classifier.predict(X_test))

# Initialize lists to store misclassified samples
misclassified_samples = []

# Loop through test samples
for i, sample in enumerate(X_test.values):
    true_class = y_test[i]

    # Predict using each classifier
    nb_prediction = naive_bayes_classifier.predict([sample])[0]
    knn_prediction = knn_classifier.predict([sample])[0]
    dt_prediction = decision_tree_classifier.predict([sample])[0]

    # Check if all classifiers misclassified the sample
    if nb_prediction != true_class and knn_prediction != true_class and dt_prediction != true_class:
        misclassified_samples.append({
            'Index': i,
            'True_Class': true_class,
            'Naive_Bayes_Prediction': nb_prediction,
            'KNN_Prediction': knn_prediction,
            'Decision_Tree_Prediction': dt_prediction
        })

# Report confusion matrices
print("Confusion Matrix for Naive Bayes Classifier:")
print(naive_bayes_confusion)
print("\nConfusion Matrix for K-Nearest Neighbors (KNN) Classifier:")
print(knn_confusion)
print("\nConfusion Matrix for Univariate Decision Tree Classifier:")
print(decision_tree_confusion)

# Report misclassified samples by all classifiers
if len(misclassified_samples) > 0:
    print("\nSamples Misclassified by All Classifiers:")
    for sample in misclassified_samples:
        print(f"Index: {sample['Index']}, True Class: {sample['True_Class']}, Naive Bayes Prediction: {sample['Naive_Bayes_Prediction']}, KNN Prediction: {sample['KNN_Prediction']}, Decision Tree Prediction: {sample['Decision_Tree_Prediction']}")
else:
    print("\nNo samples were misclassified by all classifiers.")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, auc
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt  # Add this import

# Load the data
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data"
column_names = ['ID', 'ClumpThickness', 'CellSize', 'CellShape', 'MargAdhesion', 'EpithCellSize', 'BareNuclei', 'BlandChromatin', 'NormalNucleoli', 'Mitoses', 'Class']
data = pd.read_csv(url, names=column_names)

# Drop the ID column
data.drop('ID', axis=1, inplace=True)

# Remove rows with missing values (rows containing '?')
data.replace('?', pd.NA, inplace=True)
data.dropna(inplace=True)

# Convert the 'BareNuclei' column to numeric
data['BareNuclei'] = pd.to_numeric(data['BareNuclei'])

# Separate features and target variable
X = data.drop('Class', axis=1)
y = data['Class']

# Encode the target variable 'Class' (assuming it's binary)
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize classifiers
naive_bayes_classifier = GaussianNB()
knn_classifier = KNeighborsClassifier(n_neighbors=5)
decision_tree_classifier = DecisionTreeClassifier(random_state=42)

# Train classifiers
naive_bayes_classifier.fit(X_train, y_train)
knn_classifier.fit(X_train, y_train)
decision_tree_classifier.fit(X_train, y_train)

# Calculate confusion matrices
naive_bayes_confusion = confusion_matrix(y_test, naive_bayes_classifier.predict(X_test))
knn_confusion = confusion_matrix(y_test, knn_classifier.predict(X_test))
decision_tree_confusion = confusion_matrix(y_test, decision_tree_classifier.predict(X_test))

# Calculate accuracy scores
naive_bayes_accuracy = accuracy_score(y_test, naive_bayes_classifier.predict(X_test))
knn_accuracy = accuracy_score(y_test, knn_classifier.predict(X_test))
decision_tree_accuracy = accuracy_score(y_test, decision_tree_classifier.predict(X_test))

# Calculate ROC curves and AUC scores
naive_bayes_fpr, naive_bayes_tpr, _ = roc_curve(y_test, naive_bayes_classifier.predict_proba(X_test)[:, 1])
knn_fpr, knn_tpr, _ = roc_curve(y_test, knn_classifier.predict_proba(X_test)[:, 1])
decision_tree_fpr, decision_tree_tpr, _ = roc_curve(y_test, decision_tree_classifier.predict_proba(X_test)[:, 1])

# Calculate AUC scores
naive_bayes_auc = auc(naive_bayes_fpr, naive_bayes_tpr)
knn_auc = auc(knn_fpr, knn_tpr)
decision_tree_auc = auc(decision_tree_fpr, decision_tree_tpr)

# Plot ROC curves
plt.figure(figsize=(8, 6))
plt.plot(naive_bayes_fpr, naive_bayes_tpr, label=f"Naive Bayes (AUC = {naive_bayes_auc:.2f})")
plt.plot(knn_fpr, knn_tpr, label=f"KNN (AUC = {knn_auc:.2f})")
plt.plot(decision_tree_fpr, decision_tree_tpr, label=f"Decision Tree (AUC = {decision_tree_auc:.2f})")
plt.legend()
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curves")
plt.show()

# Report confusion matrices and accuracy scores
print("Confusion Matrix for Naive Bayes Classifier:")
print(naive_bayes_confusion)
print("\nAccuracy for Naive Bayes Classifier:", naive_bayes_accuracy)

print("\nConfusion Matrix for K-Nearest Neighbors (KNN) Classifier:")
print(knn_confusion)
print("\nAccuracy for K-Nearest Neighbors (KNN) Classifier:", knn_accuracy)

print("\nConfusion Matrix for Univariate Decision Tree Classifier:")
print(decision_tree_confusion)
print("\nAccuracy for Univariate Decision Tree Classifier:", decision_tree_accuracy)